{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Edgar new data Jan 2024\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "\n",
    "seed = 2468\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The imaging is still at 30hz.\n",
    "The ephys is binarized in 1ms bins.\n",
    "The speed has corresponding triggers in imaging and ephys.\n",
    "I.e. for every entry in the speed file, there is a corresponding imaging frame or ephys bin at the given index.\n",
    "There are files for baseline and for stimulation. In each case, the imaging ROIs and the ephys units are matched.\n",
    "I.e. index 1 in baseline imaging is the same cell as index 1 is stim imaging.\n",
    "\n",
    "Please see if this format works for you. Let me know if you want me to downsample all of them to the same frame rate (30Hz) instead if that is easier for you. If this runs ok, then I can run another 1-2 mice through which should put us at 3-4 animals. I will work on getting the combinatorial paradigm in place as well, but it is a little bit complicated right now.\n",
    "'''\n",
    "parent_path = Path('./data/EB095/')\n",
    "stim_path = parent_path / \"EB095_stim/\"\n",
    "run_path = parent_path / \"EB095_run/\"\n",
    "\n",
    "\n",
    "area_ID_ephys = np.load(run_path / 'area_ID_ephys.npy')        # (106,) 8 LS, 70 ACC\n",
    "dff_run = np.load(run_path / 'dff_run.npy')                    # (294, 32727) GCaMP when running\n",
    "dff_stim = np.load(stim_path /'dff_stim.npy')                  # (294, 94696) GCaMP when stimulated \n",
    "spks_run = np.load(run_path / 'spks_run.npy')                  # (106, 32727)\n",
    "spks_stim = np.load(stim_path /'spks_stim.npy')                # (106, 94096)\n",
    "speed_run = np.load(run_path / 'speed_run.npy')                # (32727,) max 13\n",
    "\n",
    "# speed_triggers_ephys_run = np.load(run_path / 'speed_triggers_ephys_run.npy')  # (56322)\n",
    "# speed_triggers_img_run = np.load(run_path / 'speed_triggers_img_run.npy')      # (56322)\n",
    "stim_ID = np.load(stim_path /'stim_ID.npy')\n",
    "stim_ID_resolved = np.load(stim_path /'stim_ID_resolved.npy')  # (300, 6), R S or F\n",
    "stim_time = np.load(stim_path /'stim_triggers.npy')            # (300, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory analysis: Running vs Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # (106, 56322)\n",
    "plt.hist(speed_run) # (56302)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 0\n",
    "\n",
    "for lag in range(1, 21):\n",
    "    corr = []\n",
    "    for n in range(106):\n",
    "        corr.append(np.corrcoef(spks_run_30Hz_smooth[n,speed_triggers_ephys_run_30Hz[lag:56322- (20-lag)]], speed_run)[0,1])\n",
    "    plt.plot(corr[1:])\n",
    "plt.xlabel('Neuron idx')\n",
    "plt.ylabel('Correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive model fitting:\n",
    "Ignore running / stationary, fit model to both data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolve spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spks_stim_rates = rbf_rate_convolution_2d(spks_stim.T, sigma=0.1) # [93996, 106]\n",
    "n_output_neurons, n_bins = spks_stim.shape\n",
    "n_input_neurons = dff_stim.shape[0]\n",
    "spks_stim_rates = torch.hstack([torch.zeros((n_output_neurons, 50)), spks_stim_rates, torch.zeros((n_output_neurons, 50))])\n",
    "\n",
    "assert np.array(spks_stim_rates.shape).all() == np.array(spks_stim.shape).all()\n",
    "\n",
    "spks_stim_rates = spks_stim_rates.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualise a sample of ephys smooth rates\n",
    "_range = 200\n",
    "_random_start = np.random.randint(0, n_bins-_range)\n",
    "_random_neuron_idx = np.random.randint(0, n_output_neurons)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(spks_stim_rates[_random_start:_random_start+_range, _random_neuron_idx], label='smoothened rate')\n",
    "plt.bar(np.arange(200), height = spks_stim[_random_neuron_idx, _random_start:_random_start+_range] * spks_stim_rates[_random_start:_random_start+_range, _random_neuron_idx].mean().numpy(), color='r', edgecolor='r', label='spikes')\n",
    "plt.ylabel('Firing rate / Hz')\n",
    "plt.xlabel('Time / ms')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 10 time steps = 1 sigma = 330 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average firing rates of each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask1 = area_ID_ephys == 'LS'\n",
    "mask2 = area_ID_ephys != 'LS'\n",
    "\n",
    "plt.bar(np.arange(spks_stim_rates.shape[1])[mask1], spks_stim_rates.mean(0)[mask1], color='red', label='LS')\n",
    "plt.bar(np.arange(spks_stim_rates.shape[1])[mask2], spks_stim_rates.mean(0)[mask2], color='blue', label='Others')\n",
    "plt.axhline(spks_stim_rates.mean(0)[mask1].mean(), linestyle='--', color='red')\n",
    "plt.axhline(spks_stim_rates.mean(0)[mask2].mean(), linestyle='--', color='blue')\n",
    "plt.legend()\n",
    "plt.xlabel('Neuron index')\n",
    "plt.ylabel('Smoothened firing rate (Hz)')\n",
    "plt.show()\n",
    "print(f'LS neuron firing rate: {spks_stim_rates.mean(0)[mask1].mean()} Hz')\n",
    "print(f'non-LS neuron firing rate: {spks_stim_rates.mean(0)[mask2].mean()} Hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spks_stim_rates.mean(0)[mask1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsteps = 15\n",
    "cutoff_size = 2\n",
    "\n",
    "if dff_stim.shape[0] != n_bins:\n",
    "    dff_stim = dff_stim[:,:n_bins].T\n",
    "\n",
    "assert dff_stim.shape[0] == spks_stim_rates.shape[0]\n",
    "\n",
    "n_vecs = int(spks_stim_rates.shape[0] / tsteps)\n",
    "input_dim = dff_stim.shape[1]\n",
    "output_dim = spks_stim_rates.shape[1]\n",
    "n_vecs = int(spks_stim_rates.shape[0]/tsteps)\n",
    "n_stims = len(stim_ID)\n",
    "\n",
    "# pre-process data into train and test sets\n",
    "inputs, targets = torch.zeros(n_vecs, tsteps, input_dim), torch.zeros(n_vecs, 1, output_dim)\n",
    "ephys_rates = torch.as_tensor(spks_stim_rates)\n",
    "img_array = torch.as_tensor(dff_stim)\n",
    "for n in range(1, n_vecs):\n",
    "    inputs[n,:,:] = img_array[n*tsteps:(n+1)*tsteps,:]\n",
    "    targets[n,:,:] = ephys_rates[n*tsteps-3:n*tsteps-2,:]\n",
    "\n",
    "inputs = inputs[1:]\n",
    "targets = targets[1:]\n",
    "\n",
    "stim_vecs_idx = np.array([int(stim_time[i]/15) for i in range(n_stims)])\n",
    "\n",
    "# edge case: last stim happens to be in the last bin.\n",
    "stim_vecs_idx = stim_vecs_idx[:-1]\n",
    "\n",
    "non_stim_vecs_idx = []\n",
    "\n",
    "for i in range(cutoff_size, n_vecs-cutoff_size):\n",
    "    flag = True\n",
    "    for j in range(i-cutoff_size, i+cutoff_size):\n",
    "        if j in stim_vecs_idx:\n",
    "            flag = False\n",
    "    if flag:\n",
    "        non_stim_vecs_idx.append(i)\n",
    "\n",
    "non_stim_vecs_idx = [i for i in range(cutoff_size, n_vecs-cutoff_size) if i not in stim_vecs_idx] # exclude padding steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample 200 endogenous activity vectors, delete 4 vectors (4 * 15 * 33 = 2 secs) steps before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set_idx = list(stim_vecs_idx)\n",
    "\n",
    "# sample 200 endogenous data, chop off ends and add to test set\n",
    "c = 0\n",
    "while c < 200:\n",
    "    endogenous_idx = np.random.choice(non_stim_vecs_idx)\n",
    "    \n",
    "    if np.all([x in non_stim_vecs_idx for x in range(endogenous_idx-cutoff_size, endogenous_idx+cutoff_size)]):\n",
    "        c += 1\n",
    "        for i in range(endogenous_idx-cutoff_size, endogenous_idx+cutoff_size):\n",
    "            non_stim_vecs_idx.remove(i)\n",
    "        test_set_idx.append(endogenous_idx)\n",
    "    \n",
    "X_train, Y_train = inputs[non_stim_vecs_idx], targets[non_stim_vecs_idx]\n",
    "X_test, Y_test = inputs[test_set_idx], targets[test_set_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for LS neurons only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LS_neuron_idx = np.where(area_ID_ephys == 'LS')[0]\n",
    "ACC_neuron_idx = np.where(area_ID_ephys == 'ACC')[0]\n",
    "Others_neuron_idx = np.where(area_ID_ephys == 'Other')[0]\n",
    "nLS_neuron_idx = np.where(area_ID_ephys != 'LS')[0]\n",
    "\n",
    "forward_mse_idx = list(np.nonzero(np.array(stim_ID) == 'F')[0])\n",
    "backward_mse_idx = list(np.nonzero(np.array(stim_ID) == 'R')[0])\n",
    "random_mse_idx = list(np.nonzero(np.array(stim_ID) == 'S')[0])\n",
    "non_stim_mse_idx = np.arange(len(stim_ID), Y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first 13 neurons are lateral septal according to Edgar\n",
    "Y_train_LS = Y_train[..., LS_neuron_idx]\n",
    "Y_test_LS = Y_test[..., LS_neuron_idx]\n",
    "output_dim = Y_train_LS.shape[-1]\n",
    "\n",
    "batch_size = 4096\n",
    "train_dataset = BNN_Dataset(X_train, Y_train_LS)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "test_dataset = BNN_Dataset(X_test, Y_test_LS)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), drop_last=False, shuffle=False)\n",
    "# valid_dataset = BNN_Dataset(X_valid, Y_valid_LS)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=len(valid_dataset), drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_bayes_opt = False\n",
    "from bayes_opt import BayesianOptimization\n",
    "from models import TransformerOneStep as TransformerDNN\n",
    "from train import train_transformer, eval_transformer\n",
    "\n",
    "def black_box_function(d_model, num_heads, hidden_dim, n_layers, seed=seed):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    seed = 123\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    hidden_dim, d_model, n_layers, num_heads = int(hidden_dim), int(d_model), int(n_layers), int(num_heads)\n",
    "    d_model = int(int(d_model/num_heads)*num_heads) # d_model must be multiples of n_heads\n",
    "    if d_model %2 != 0:\n",
    "        d_model += num_heads\n",
    "        \n",
    "    DNN = TransformerDNN(input_dim=input_dim,\n",
    "                        d_model=d_model,\n",
    "                        num_heads=num_heads,\n",
    "                        hidden_dim=hidden_dim,\n",
    "                        output_dim=output_dim,\n",
    "                        n_layers=n_layers,\n",
    "                        device=device,\n",
    "                        pos_output=True,\n",
    "                        use_mask=True,\n",
    "                        dropout=0.1,\n",
    "                        bin_output=False,\n",
    "                        softmax_output=False).to(device)\n",
    "    \n",
    "    optimiser = torch.optim.Adam(DNN.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, gamma=0.99)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=0.5, patience=25, threshold=0.01)\n",
    "\n",
    "\n",
    "    return_dict = train_transformer(\n",
    "        model=DNN,\n",
    "        train_loader=train_dataloader, test_loader=test_dataloader, # use valid loader here\n",
    "        optimiser=optimiser, criterion=criterion, num_epochs=250,\n",
    "        verbose=False, force_stop=False, scheduler=scheduler)\n",
    "\n",
    "    return -min(return_dict['eval_losses'])\n",
    "\n",
    "def run_bayes_opt(pbounds, init_points=100, n_iter=100):\n",
    "    optimizer = BayesianOptimization(f=black_box_function, pbounds=pbounds, random_state=1)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "\n",
    "if run_bayes_opt:\n",
    "    run_bayes_opt(pbounds = {'d_model':(100, 1000),\n",
    "               'num_heads':(1, 20),\n",
    "               'hidden_dim':(100, 1000),\n",
    "               'n_layers':(1, 5)}, init_points=50, n_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMER\n",
    "# |   iter    |  target   |  d_model  | hidden... | n_layers  | num_heads |\n",
    "# | 62        | -163.6    | 221.1     | 848.9     | 5.0       | 20.0      |\n",
    "# | 57        | -164.1    | 270.0     | 876.5     | 3.016     | 16.14     |\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from models import TransformerOneStep as TransformerDNN\n",
    "DNN_LS = TransformerDNN(input_dim=input_dim,\n",
    "                    d_model=220,\n",
    "                    num_heads=20,\n",
    "                    hidden_dim=848,\n",
    "                    output_dim=output_dim,\n",
    "                    n_layers=5,\n",
    "                    device=device,\n",
    "                    pos_output=True,\n",
    "                    use_mask=False,\n",
    "                    dropout=0.1,\n",
    "                    bin_output=False,\n",
    "                    softmax_output=False).to(device)\n",
    "\n",
    "\n",
    "assert len(np.intersect1d(forward_mse_idx, backward_mse_idx)) == 0\n",
    "assert len(np.intersect1d(backward_mse_idx, random_mse_idx)) == 0\n",
    "assert len(np.intersect1d(random_mse_idx, non_stim_mse_idx)) == 0\n",
    "assert len(np.intersect1d(non_stim_mse_idx, forward_mse_idx)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GLM for LS neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "# fit GLM with identity link function\n",
    "X_train_np = X_train.cpu().numpy().reshape(X_train.shape[0],-1)\n",
    "Y_train_LS_np = Y_train_LS.cpu().numpy().reshape(Y_train_LS.shape[0],-1) + 1e-6 # numerical stability\n",
    "\n",
    "X_test_np = X_test.cpu().numpy().reshape(X_test.shape[0],-1)\n",
    "Y_test_LS_np = Y_test_LS.cpu().numpy().reshape(Y_test_LS.shape[0],-1) + 1e-6 # numerical stability\n",
    "\n",
    "\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "# pred for each neuron\n",
    "glm_LS_losses = np.zeros_like(Y_test_LS_np) # (499, 8)\n",
    "output_dim = Y_test_LS_np.shape[1]\n",
    "\n",
    "for i in range(output_dim):\n",
    "    glm_LS_reg = TweedieRegressor(power=0, alpha=1, link='identity')\n",
    "    glm_LS_reg.fit(X_train_np, Y_train_LS_np[:,i])\n",
    "    \n",
    "    glm_LS_losses[:,i] += np.power(Y_test_LS_np[:,i] - glm_LS_reg.predict(X_test_np), 2)\n",
    "    logger.info(f'Loss for LS neuron No {i}: {glm_LS_losses[:,i].sum() / Y_test_LS_np.shape[0]} (Hz^2).')\n",
    "    logger.info(f'Forward: {glm_LS_losses[forward_mse_idx,i].sum() / len(forward_mse_idx)}, Backward: {glm_LS_losses[backward_mse_idx,i].sum() / len(backward_mse_idx)}, Random: {glm_LS_losses[random_mse_idx, i].sum() / len(random_mse_idx)}. Endogenous: {glm_LS_losses[non_stim_mse_idx, i].sum() / len(non_stim_mse_idx)}')\n",
    "\n",
    "logger.info(f'Pred mean overall loss: {np.power(Y_test_LS_np - Y_test_LS_np.mean(0), 2).sum() / np.prod(Y_test_LS_np.shape)}')\n",
    "logger.info(f'GLM with identity function overall loss: {glm_LS_losses.sum() / np.prod(Y_test_LS.shape)}')\n",
    "\n",
    "logger.info(f'GLM prediction for forward stim R2 loss: {glm_LS_losses[forward_mse_idx].sum()/len(forward_mse_idx)/output_dim}')\n",
    "logger.info(f'GLM prediction for backward stim R2 loss: {glm_LS_losses[backward_mse_idx].sum() / len(backward_mse_idx)/output_dim}')\n",
    "logger.info(f'GLM prediction for random stim R2 loss: {glm_LS_losses[random_mse_idx].sum() / len(random_mse_idx)/output_dim}')\n",
    "logger.info(f'GLM prediction for non-stim stim R2 loss: {glm_LS_losses[non_stim_mse_idx].sum() / len(non_stim_mse_idx)/output_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f'Predicting mean for each of the 8 LS neurons: {np.power(Y_test_LS_np - Y_test_LS_np.mean(0), 2).sum(0) / Y_test_LS_np.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(8)-0.2, np.sqrt(glm_LS_losses.mean(0)), width=0.1, label='overall')\n",
    "plt.bar(np.arange(8)-0.1, np.sqrt(glm_LS_losses[forward_mse_idx].mean(0)), width=0.1, label='forward')\n",
    "plt.bar(np.arange(8), np.sqrt(glm_LS_losses[backward_mse_idx].mean(0)), width=0.1, label='backward')\n",
    "plt.bar(np.arange(8)+0.1, np.sqrt(glm_LS_losses[random_mse_idx].mean(0)), width=0.1, label='random')\n",
    "plt.bar(np.arange(8)+0.2, np.sqrt(glm_LS_losses[non_stim_mse_idx].mean(0)), width=0.1, label='endogenous')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GLM for Others neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_train_nLS = Y_train[..., nLS_neuron_idx]\n",
    "Y_test_nLS = Y_test[..., nLS_neuron_idx]\n",
    "# Y_valid_nLS = Y_valid[..., 13:]\n",
    "output_dim = Y_train_nLS.shape[-1]\n",
    "\n",
    "batch_size = 2048\n",
    "train_dataset = BNN_Dataset(X_train, Y_train_nLS)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "test_dataset = BNN_Dataset(X_test, Y_test_nLS)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), drop_last=False, shuffle=False)\n",
    "\n",
    "# valid_dataset = BNN_Dataset(X_valid, Y_valid_nLS)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=len(valid_dataset), drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit GLM with identity link function\n",
    "X_train_np = X_train.cpu().numpy().reshape(X_train.shape[0],-1)\n",
    "Y_train_nLS_np = Y_train_nLS.cpu().numpy().reshape(Y_train_nLS.shape[0],-1) + 1e-6 # numerical stability\n",
    "\n",
    "X_test_np = X_test.cpu().numpy().reshape(X_test.shape[0],-1)\n",
    "Y_test_nLS_np = Y_test_nLS.cpu().numpy().reshape(Y_test_nLS.shape[0],-1) + 1e-6 # numerical stability\n",
    "\n",
    "output_dim = Y_test_nLS.shape[-1]\n",
    "\n",
    "# pred for each neuron\n",
    "glm_nLS_losses = np.zeros_like(Y_test_nLS_np)\n",
    "output_dim = Y_test_nLS_np.shape[1]\n",
    "\n",
    "for i in tqdm(range(output_dim)):\n",
    "    glm_nLS_reg = TweedieRegressor(power=0, alpha=1, link='identity')\n",
    "    glm_nLS_reg.fit(X_train_np, Y_train_nLS_np[:,i])\n",
    "    \n",
    "    glm_nLS_losses[:,i] += np.power(Y_test_nLS_np[:,i] - glm_nLS_reg.predict(X_test_np), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
